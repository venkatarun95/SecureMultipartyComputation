import argparse
import os
import pickle
import random
import re
import subprocess
import sys
import threading
import time

config = {
    'classpath': '../assets/Scapi-2.4.jar::../assets/commons-exec-1.2.jar:../assets/bcprov-jdk16-146.jar:../lib/hamcrest-core-1.3.jar:../assets/activemq-all-5.9.1.jar:../lib/jpbc-api-2.0.0.jar:../lib/jpbc-plaf-2.0.0.jar:/usr/share/java/mysql-connector-java.jar:../src',
    'lib_path': '../assets/:/usr/ssl/lib/',
    'keys_per_file': 5 # Number of keys stored in each key file
}

# To be decided at run-time
# - parallelism - number of server replicas
# - server_addrs - read from a file generated by setup-servers.py
# - mode - one of ['register', 'file'] denoting which operation to request
# - keydir - directory in which keys are stored
# - num_requests - number of requests to make per replica
params = {}

def init_params():
    parser = argparse.ArgumentParser('python run-client.py')
    parser.add_argument('-m', '--mode', help='Mode to operate in', choices=['register', 'file'])
    parser.add_argument('-c', '--config-file', help='File containing server information')
    parser.add_argument('-d', '--key-dir', help='Directory where key files are stored/to be stored')
    parser.add_argument('-n', '--num-req', help='Number of requests to make', type=int, default=10)
    parser_prime = parser.add_mutually_exclusive_group(required=False)
    parser_prime.add_argument('--prime', dest='prime', action='store_true', help="Prime the servers to pre-create buckets and avoid race condition")
    parser_prime.add_argument('--no-prime', dest='prime', action='store_false')
    parser.set_defaults(prime=True)
    args = parser.parse_args()

    f = open(args.config_file, 'r')
    params['server_addrs'] = pickle.load(f)
    params['parallelism'] = len(params['server_addrs'])
    params['mode'] = args.mode
    params['keydir'] = args.key_dir
    params['num_requests'] = args.num_req
    params['prime'] = args.prime

class RunClient (threading.Thread):
    def __init__(self, replica_id):
        threading.Thread.__init__(self)
        self.replica_id = replica_id
        self.__update_key_cache_file()

    def __update_key_cache_file(self):
        '''Load/create the file and dictionary storing How many keys are available in
        which key file

        This is stored in self.key_cache_filename. The format is a dictionary
        with (key - key file name, value - number of keys remaining)
        '''
        self.key_cache_filename = os.path.join(params['keydir'],
                                               'key-cache-%d' % self.replica_id)
        if os.path.isfile(self.key_cache_filename):
            key_cache_file = open(self.key_cache_filename, 'r')
            self.key_cache = pickle.load(key_cache_file)
        else:
            self.key_cache = {}
        re_keyfile = re.compile('.*/key-([0-9]*)-([0-9]*).key')
        for fname in os.listdir(params['keydir']):
            fname = os.path.join(params['keydir'], fname)
            match_keyfile = re_keyfile.match(fname)
            if match_keyfile is None:
                continue
            if fname in self.key_cache:
                continue
            if self.replica_id == int(match_keyfile.groups()[0]):
                self.key_cache[fname] = config['keys_per_file']
        pickle.dump(self.key_cache, open(self.key_cache_filename, 'w'))


    def __get_keyfile(self):
        '''Returns the name of a keyfile that has unused keys. If no such file exists,
        returns None.

        Also updates self.key_cache and the appropriate pickle file.
        '''
        for key_filename in self.key_cache:
            if self.key_cache[key_filename] > 0:
                self.key_cache[key_filename] -= 1
                pickle.dump(self.key_cache, open(self.key_cache_filename, 'w'))
                return key_filename
        print("Ran out of registered keys!")
        exit(1)
        return None

    def __gen_rnd_allegations(self, num_alleg):
        res = []
        while len(res) < num_alleg:
            thresh = random.expovariate(1.0 / 5.0)
            thresh = max(min(int(thresh), 4), 2)
            # num_copies = thresh
            # if random.random() < 0.5:
            #     num_copies -= 1
            num_copies = 1
            meta_data = random.randint(0, 1000000)
            res += [(thresh, meta_data, random.randint(0, 1000)) for _ in range(num_copies)]
        random.shuffle(res)
        return res[:num_alleg]

    def run(self):
        # Prepare the address string
        addr_str = '::'.join(['%s:%d' % (x[0], x[1]) for x in params['server_addrs'][self.replica_id]])
        start_time = time.time()
        allegations = self.__gen_rnd_allegations(params['num_requests'])
        for req in range(params['num_requests']):
            if params['mode'] == 'register':
                key_file_id = req
                self.__update_key_cache_file()
                while True:
                    key_filename = os.path.join(params['keydir'], 'key-%d-%d.key' % (self.replica_id, key_file_id))
                    if key_filename not in self.key_cache:
                        break
                    key_file_id += 1
                args = ['register', key_filename]
            elif params['mode'] == 'file':
                key_filename = self.__get_keyfile()
                alleg = allegations[req]
                threshold = alleg[0]
                meta_data = alleg[1]
                allegation = alleg[2]
                args = ['file', key_filename, str(threshold), str(meta_data), str(allegation)]

            req_start_time = time.time()
            cmd = ['java', '-classpath', config['classpath'],
                   '-Djava.net.preferIPv4Stack=true',
                   '-Djava.security.egd=file:/dev/./urandom',
                   '-Djava.library.path=%s' % config['lib_path'],
                   'client.Client', addr_str] + args
            subprocess.call(cmd)
            req_elapsed_time = time.time() - req_start_time
            tot_elapsed_time = time.time() - start_time
            print("PERF REPID %d REQID %d LAT %f TPT %f" % (self.replica_id, req, req_elapsed_time, (req + 1) / tot_elapsed_time))
            sys.stdout.flush()

    def prime_buckets(self, max_bucket):
        '''Files useless allegations into buckets so that they are created by the
        servers. Helps avoid race-conditions later
        '''
        addr_str = '::'.join(['%s:%d' % (x[0], x[1]) for x in params['server_addrs'][0]])
        for thresh in range(2, max_bucket):
            key_filename = self.__get_keyfile()
            args = ['file', key_filename, str(thresh), '1', 'priming-allegation']
            req_start_time = time.time()
            cmd = ['java', '-classpath', config['classpath'],
                   '-Djava.net.preferIPv4Stack=true',
                   '-Djava.library.path=%s' % config['lib_path'],
                   'client.Client', addr_str] + args
            subprocess.call(cmd)
            req_elapsed_time = time.time() - req_start_time
            print("PERF REPID %d THRESH %d LAT %f" % (self.replica_id, thresh, req_elapsed_time))

if __name__ == "__main__":
    init_params()
    clients = []
    for replica_id in range(params['parallelism']):
        client = RunClient(replica_id)
        clients.append(client)

    if params['prime'] and params['mode'] == 'file':
        clients[0].prime_buckets(5)
    for client in clients:
        client.start()

    for client in clients:
        client.join()
